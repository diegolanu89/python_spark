{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark \n",
    "findspark.init('C:\\Spark\\spark-3.0.0-preview2-bin-hadoop2.7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "try: \n",
    "    type(sc)\n",
    "except NameError:\n",
    "    sc = pyspark.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = [\n",
    "    (1, 1, '01', '2017', 500), # sera la minima\n",
    "    (1, 2, '01', '2017', 500),\n",
    "    (1, 2, '01', '2017', 500),\n",
    "    (1, 2, '01', '2017', 500),\n",
    "    (1, 2, '01', '2017', 500),\n",
    "    (1, 1, '01', '2016', 500),\n",
    "    (1, 2, '01', '2016', 500),\n",
    "    (1, 2, '01', '2016', 500),\n",
    "    (1, 2, '01', '2016', 500),\n",
    "    (1, 2, '01', '2016', 500),\n",
    "    (2, 3, '01', '2017', 500),\n",
    "    (2, 3, '01', '2017', 500),\n",
    "    (2, 3, '01', '2017', 500),\n",
    "    (2, 3, '01', '2017', 500),\n",
    "    (2, 3, '01', '2017', 500),\n",
    "    (4, 3, '01', '2017', 500),\n",
    "    (4, 3, '01', '2017', 500),\n",
    "    (4, 3, '02', '2017', 500),\n",
    "    (4, 3, '03', '2017', 500),    \n",
    "\n",
    "]\n",
    "\n",
    "stores = [\n",
    "    (1 , 'address 1', -1, -1, '30002', 'Georgia'),\n",
    "    (2 , 'address 2', -2, -2, '30003', 'Georgia'),\n",
    "    (3 , 'address 2', -3, -3, '30004', 'Georgia'),\n",
    "    (4 , 'address 2', -4, -4, '10119', 'New York')    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener la tienda que realizó menor cantidad de ventas en el estado de \"Georgia\" en todo el año 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "salesRdd = sc.parallelize(sales)\n",
    "storesRdd = sc.parallelize(stores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtramos la informacion de 2017 de ventas y realizamos un cambio de clave\n",
    "salesToJoin = salesRdd.filter(lambda x: x[3] == '2017')\\\n",
    "    .map(lambda x: (x[1], (x[1],x[4]))) # (K,V) K: id_tienda, V: (id_tienda, total_ventas_mensuales)\n",
    "\n",
    "# consideramos solo tiendas de Georgia\n",
    "storesToJoin = storesRdd.filter(lambda x: x[5] == 'Georgia')\\\n",
    "    .map(lambda x: (x[0], x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(1, 500)"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "# realizamos el join de ambos data frames\n",
    "# simplificamos el formato para evitar los reduce complejos\n",
    "# sumarizamos cantidad de ventas por tienda\n",
    "# buscamos el minimo con reduce\n",
    "salesToJoin.join(storesToJoin)\\\n",
    "    .map(lambda x: (x[0], x[1][0][1]))\\\n",
    "    .reduceByKey(lambda x, y: x+y)\\\n",
    "    .reduce(lambda x, y: x if x[1] < y[1] else y)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37064bitpythonpipenv7230673d0c4d4573af2fbc53a31cd976",
   "display_name": "Python 3.7.0 64-bit ('PYTHON': pipenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}